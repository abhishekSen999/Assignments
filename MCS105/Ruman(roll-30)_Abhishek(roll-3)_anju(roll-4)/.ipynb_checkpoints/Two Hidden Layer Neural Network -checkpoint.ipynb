{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Hidden Layer Neural Network\n",
    "\n",
    "\n",
    "- Implementing a 2-class classification neural network with a two hidden layer\n",
    "- Using units with a non-linear activation function, such as tanh/sigmoid / relu \n",
    "- Computeing the cross entropy loss \n",
    "- Implementing forward and backward propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "Let's first import all the packages that you will need during this assignment.\n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [sklearn](http://scikit-learn.org/stable/) provides simple and efficient tools for data mining and data analysis. \n",
    "- [matplotlib](http://matplotlib.org) is a library for plotting graphs in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import sklearn.linear_model\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1) # set a seed so that the results are consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dataset ##\n",
    "\n",
    "First, let's get the dataset you will work on. The following code will load a \"flower\" 2-class dataset into variables `X` and `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module sklearn.datasets.olivetti_faces in sklearn.datasets:\n",
      "\n",
      "NAME\n",
      "    sklearn.datasets.olivetti_faces - Modified Olivetti faces dataset.\n",
      "\n",
      "DESCRIPTION\n",
      "    The original database was available from\n",
      "    \n",
      "        http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\n",
      "    \n",
      "    The version retrieved here comes in MATLAB format from the personal\n",
      "    web page of Sam Roweis:\n",
      "    \n",
      "        http://www.cs.nyu.edu/~roweis/\n",
      "    \n",
      "    There are ten different images of each of 40 distinct subjects. For some\n",
      "    subjects, the images were taken at different times, varying the lighting,\n",
      "    facial expressions (open / closed eyes, smiling / not smiling) and facial\n",
      "    details (glasses / no glasses). All the images were taken against a dark\n",
      "    homogeneous background with the subjects in an upright, frontal position (with\n",
      "    tolerance for some side movement).\n",
      "    \n",
      "    The original dataset consisted of 92 x 112, while the Roweis version\n",
      "    consists of 64x64 images.\n",
      "\n",
      "FUNCTIONS\n",
      "    fetch_olivetti_faces(data_home=None, shuffle=False, random_state=0, download_if_missing=True)\n",
      "        Loader for the Olivetti faces data-set from AT&T.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <olivetti_faces>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : optional, default: None\n",
      "            Specify another download and cache folder for the datasets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        shuffle : boolean, optional\n",
      "            If True the order of the dataset is shuffled to avoid having\n",
      "            images of the same person grouped.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=0)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        download_if_missing : optional, True by default\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        An object with the following attributes:\n",
      "        \n",
      "        data : numpy array of shape (400, 4096)\n",
      "            Each row corresponds to a ravelled face image of original size\n",
      "            64 x 64 pixels.\n",
      "        \n",
      "        images : numpy array of shape (400, 64, 64)\n",
      "            Each row is a face image corresponding to one of the 40 subjects\n",
      "            of the dataset.\n",
      "        \n",
      "        target : numpy array of shape (400, )\n",
      "            Labels associated to each face image. Those labels are ranging from\n",
      "            0-39 and correspond to the Subject IDs.\n",
      "        \n",
      "        DESCR : string\n",
      "            Description of the modified Olivetti Faces Dataset.\n",
      "        \n",
      "        Notes\n",
      "        ------\n",
      "        \n",
      "        This dataset consists of 10 pictures each of 40 individuals. The original\n",
      "        database was available from (now defunct)\n",
      "        \n",
      "            http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\n",
      "        \n",
      "        The version retrieved here comes in MATLAB format from the personal\n",
      "        web page of Sam Roweis:\n",
      "        \n",
      "            http://www.cs.nyu.edu/~roweis/\n",
      "    \n",
      "    remove(path, *, dir_fd=None)\n",
      "        Remove a file (same as unlink()).\n",
      "        \n",
      "        If dir_fd is not None, it should be a file descriptor open to a directory,\n",
      "          and path should be relative; path will then be relative to that directory.\n",
      "        dir_fd may not be implemented on your platform.\n",
      "          If it is unavailable, using it will raise a NotImplementedError.\n",
      "\n",
      "DATA\n",
      "    FACES = RemoteFileMetadata(filename='olivettifaces.mat',...c62d3e1266e...\n",
      "    MODULE_DOCS = 'Modified Olivetti faces dataset.\\n\\nThe original d...il...\n",
      "\n",
      "FILE\n",
      "    c:\\programdata\\anaconda3\\lib\\site-packages\\sklearn\\datasets\\olivetti_faces.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For help on the dataset of sklearn library\n",
    "help(sklearn.datasets.olivetti_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.12547421 -0.08789608 -0.07544964 ... -0.19328208 -0.18543996\n",
      "  -0.19439802]\n",
      " [ 0.07567523  0.04871595  0.04624351 ... -0.19328208 -0.19549021\n",
      "  -0.19963364]\n",
      " [-0.11397997 -0.04418021  0.01978849 ... -0.2075001  -0.20051533\n",
      "  -0.19963364]\n",
      " ...\n",
      " [ 0.1388936   0.1306832   0.16793665 ... -0.16484606 -0.20051533\n",
      "  -0.15251322]\n",
      " [-0.25765812 -0.28461736 -0.32941788 ...  0.29013026  0.33717304\n",
      "   0.371047  ]\n",
      " [ 0.16188207  0.037787   -0.2500528  ...  0.04368475  0.05074093\n",
      "   0.0935601 ]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = fetch_olivetti_faces()\n",
    "\n",
    "X = data.data\n",
    "Y = data.target\n",
    "\n",
    "#Normalize the dataset\n",
    "# X = X.T\n",
    "X = (X-np.mean(X, axis=0, keepdims = True))/(np.max(X, axis=0, keepdims = True)-np.min(X, axis = 0, keepdims = True))\n",
    "\n",
    "#map labels 0 or 1\n",
    "Y[Y<=0]=0\n",
    "Y[Y>0]=1\n",
    "\n",
    "# choose indices from 0 and 1 seperately\n",
    "class0 = [i for i in range(0, len(Y)) if(Y[i]==0)]\n",
    "class1 = [i for i in range(0, len(Y)) if(Y[i]==1)]\n",
    "\n",
    "# sampling indices from 0 and 1 classes seperately\n",
    "train_indices = random.sample(class0, int(0.7*len(class0))) + random.sample(class1, int(0.7*len(class1)))\n",
    "\n",
    "# test indices: all indices not used in training\n",
    "test_indices = [i for i in range(0, len(Y)) if i not in train_indices]\n",
    "\n",
    "# training data\n",
    "X_train = X[train_indices]\n",
    "Y_train = Y[train_indices].reshape(len(train_indices), 1)\n",
    "\n",
    "# print(X_train.shape, len(train_indices))\n",
    "# testing data\n",
    "X_test = X[test_indices]\n",
    "Y_test = Y[test_indices].reshape(len(test_indices), 1)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X(Features) is: (280, 4096)\n",
      "The shape of Y(Target values) is: (280, 1)\n",
      "Number of training examples: 4096\n"
     ]
    }
   ],
   "source": [
    "shape_X = X_train.shape\n",
    "# Y = Y.reshape(1,Y.shape[0])\n",
    "shape_Y = Y_train.shape\n",
    "m = X_train.shape[1]  # training set size\n",
    "\n",
    "# Type: numpy array, validate using function type(X), type(Y)\n",
    "print ('The shape of X(Features) is: ' + str(shape_X))\n",
    "print ('The shape of Y(Target values) is: ' + str(shape_Y))\n",
    "print ('Number of training examples:', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y, h_layers,h1,h2):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    h_layers --number of hidden layers\n",
    "    \"\"\" \n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h1 = h1#20\n",
    "    n_h2 = h2#10\n",
    "    n_y = Y.shape[0] # size of output layer\n",
    "    ### END CODE HERE ###\n",
    "    return list([n_x, n_h1, n_h2, n_y])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize parameters of 3 layer Neural network with 2 hidden layers\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Input : layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    Output: python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Forward and Backward Propagation ####\n",
    "\n",
    "**Question**: Implement `forward_propagation()`.\n",
    "\n",
    "**Instructions**:\n",
    "- Look above at the mathematical representation of your classifier.\n",
    "- You can use the function `np.tanh()`. It is part of the numpy library.\n",
    "- The steps you have to implement are:\n",
    "    1. Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()`) by using `parameters[\"..\"]`.\n",
    "    2. Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).\n",
    "- Values needed in the backpropagation are stored in \"`cache`\". The `cache` will be given as an input to the backpropagation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "activations_forward = {\n",
    "    \"tanh\": lambda x: np.tanh(x),\n",
    "    \"sigmoid\": lambda x: 1.0/(1+np.exp(-x)),\n",
    "    \"relu\": lambda x: np.maximum(0, x)\n",
    "}\n",
    "activations_backward = {\n",
    "    \"tanh\": lambda x: 1-np.power(x, 2),\n",
    "    \"sigmoid\": lambda x: np.multiply(x, 1-x),\n",
    "    \"relu\": lambda x: np.greater(x, 0).astype(int)\n",
    "}\n",
    "\n",
    "def forward_propagation(X, parameters, functions=[\"tanh\", \"tanh\", \"sigmoid\"]):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = activations_forward[functions[0]](Z1)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = activations_forward[functions[1]](Z2)\n",
    "    Z3 = np.dot(W3,A2) + b3\n",
    "    A3 = activations_forward[functions[2]](Z3)\n",
    "\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2,\n",
    "             \"Z3\": Z3,\n",
    "             \"A3\": A3}\n",
    "    \n",
    "    return A3, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "#Compute Cost\n",
    "def compute_cost(A_final, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cost \n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the final activation\n",
    "    Y -- \"true\" labels vector\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 , b2 , W3 , b3\n",
    "    \n",
    "    Returns:\n",
    "    cost \n",
    "    \"\"\"\n",
    "    #print(\"A_final\",(A_final))\n",
    "    m = Y.shape[1] # number of example\n",
    "    # Compute the cost\n",
    "    \n",
    "    logprobs = np.multiply(np.log(A_final), Y) + np.multiply((1 - Y), np.log(1 - A_final))\n",
    "    cost = -np.sum(logprobs) / m\n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    return cost\n",
    "\n",
    "def dCost(A, y):\n",
    "    return np.divide(1-y, 1-A) - np.divide(y, A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: backward_propagation\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y, functions=[\"tanh\", \"tanh\", \"sigmoid\"]):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" , \"A2\" , Z3 , A3.\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 , W2 and W3 from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Retrieve also A1 , A2 and A3 from dictionary \"cache\".\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    A3 = cache[\"A3\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2 , dW3, db3. \n",
    "    dZ3 = np.multiply(dCost(A3, Y), activations_backward[functions[2]](A3))\n",
    "    dW3 = 1/m*(np.dot(dZ3,A2.T))\n",
    "    db3 = 1/m*(np.sum(dZ3,axis=1, keepdims=True))\n",
    "    \n",
    "    dZ2 = np.multiply(np.dot(W3.T,dZ3),activations_backward[functions[1]](A2))\n",
    "    dW2 = 1/m*(np.dot(dZ2,A1.T))\n",
    "    db2 = 1/m*(np.sum(dZ2,axis=1, keepdims=True))\n",
    "    \n",
    "    dZ1 = np.multiply(np.dot(W2.T,dZ2),activations_backward[functions[0]](A1))\n",
    "    dW1 = 1/m*(np.dot(dZ1,X.T))\n",
    "    db1 = 1/m*(np.sum(dZ1,axis=1, keepdims=True))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2,\n",
    "             \"dW3\": dW3,\n",
    "             \"db3\": db3}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 0.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    dW3 = grads[\"dW3\"]\n",
    "    db3 = grads[\"db3\"]\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    W3 = W3 - learning_rate*dW3\n",
    "    b3 = b3 - learning_rate*db3\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: nn_model\n",
    "\n",
    "def nn_model(X, Y, h1=20, h2=10, functions=[\"tanh\", \"tanh\", \"sigmoid\"], num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    layer_dims = layer_sizes(X, Y , 2, h1, h2)\n",
    "    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    ### START CODE HERE ### (≈ 5 lines of code)\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    costList = []\n",
    "    for i in range(0, num_iterations+1):\n",
    "        \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A3, cache\".\n",
    "        A3, cache = forward_propagation(X,parameters, functions)\n",
    "        \n",
    "        # Cost function. Inputs: \"A3, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A3,Y,parameters)\n",
    "        costList.append(cost)\n",
    "        \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters,cache,X,Y, functions)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters,grads)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    plt.plot(range(0, num_iterations+1), costList)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(parameters, X, functions=[\"tanh\", \"tanh\", \"sigmoid\"]):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A3, cache = forward_propagation(X,parameters, functions)\n",
    "    predictions = 1*(A3>0.5)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693240\n",
      "Cost after iteration 100: 0.132736\n",
      "Cost after iteration 200: 0.119205\n",
      "Cost after iteration 300: 0.116384\n",
      "Cost after iteration 400: 0.097000\n",
      "Cost after iteration 500: 0.053335\n",
      "Cost after iteration 600: 0.020276\n",
      "Cost after iteration 700: 0.009399\n",
      "Cost after iteration 800: 0.005632\n",
      "Cost after iteration 900: 0.003899\n",
      "Cost after iteration 1000: 0.002940\n",
      "Cost after iteration 1100: 0.002342\n",
      "Cost after iteration 1200: 0.001937\n",
      "Cost after iteration 1300: 0.001646\n",
      "Cost after iteration 1400: 0.001429\n",
      "Training over\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a model with a n_h-dimensional hidden layer# with nodes 20,10\n",
    "activations = [\"tanh\", \"sigmoid\", \"sigmoid\"]\n",
    "\n",
    "parameters = nn_model(X_train.T, Y_train.T, 10, 5, activations, num_iterations = 1400, print_cost=True)\n",
    "print(\"Training over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "predictions = predict(parameters, X_test.T, activations)\n",
    "#print(predictions)\n",
    "# print( Y_test.shape, predictions.shape)\n",
    "# np.dot(Y_test, predictions)\n",
    "print ('Accuracy: %d' % float((np.dot(predictions, Y_test) + np.dot(1-predictions, 1-Y_test))/float(Y_test.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,Y_train)\n",
    "print(\"LogisticRegression\", model.score(X_test,Y_test)*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
